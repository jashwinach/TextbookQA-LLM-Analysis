{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT4 connection and prompting with API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Custom OpenAI API key\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Focusing on Image-Question-Answering first using the TQA dataset </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lesson_name</th>\n",
       "      <th>question_name</th>\n",
       "      <th>answer_choice_1</th>\n",
       "      <th>answer_choice_2</th>\n",
       "      <th>answer_choice_3</th>\n",
       "      <th>answer_choice_4</th>\n",
       "      <th>correct_answer</th>\n",
       "      <th>image_path</th>\n",
       "      <th>image_has_labels_to_guess</th>\n",
       "      <th>caption</th>\n",
       "      <th>gpt4_generated_answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>climate and its causes</td>\n",
       "      <td>Which label refers to rains?</td>\n",
       "      <td>V</td>\n",
       "      <td>T</td>\n",
       "      <td>U</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>../Dataset/test/abc_question_images/rain_shado...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>a diagram of the water cycle</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>climate and its causes</td>\n",
       "      <td>How does water from the clouds reach the land ...</td>\n",
       "      <td>ICE FROM THE MOUNTAIN PEAK</td>\n",
       "      <td>AS RAIN</td>\n",
       "      <td>WIND</td>\n",
       "      <td>GRASS</td>\n",
       "      <td>AS RAIN</td>\n",
       "      <td>../Dataset/test/abc_question_images/rain_shado...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>a diagram of the water cycle</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>climate and its causes</td>\n",
       "      <td>What letter represents the condensation process?</td>\n",
       "      <td>W</td>\n",
       "      <td>J</td>\n",
       "      <td>H</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>../Dataset/test/abc_question_images/rain_shado...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>a diagram of the water cycle</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>climate and its causes</td>\n",
       "      <td>Where can you find moist air?</td>\n",
       "      <td>W</td>\n",
       "      <td>A</td>\n",
       "      <td>H</td>\n",
       "      <td>J</td>\n",
       "      <td>J</td>\n",
       "      <td>../Dataset/test/abc_question_images/rain_shado...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>a diagram of the water cycle</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>climate and its causes</td>\n",
       "      <td>Where is condensation?</td>\n",
       "      <td>T</td>\n",
       "      <td>H</td>\n",
       "      <td>A</td>\n",
       "      <td>W</td>\n",
       "      <td>T</td>\n",
       "      <td>../Dataset/test/abc_question_images/rain_shado...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>a diagram of the water cycle</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              lesson_name                                      question_name  \\\n",
       "0  climate and its causes                       Which label refers to rains?   \n",
       "1  climate and its causes  How does water from the clouds reach the land ...   \n",
       "2  climate and its causes   What letter represents the condensation process?   \n",
       "3  climate and its causes                      Where can you find moist air?   \n",
       "4  climate and its causes                             Where is condensation?   \n",
       "\n",
       "              answer_choice_1 answer_choice_2 answer_choice_3 answer_choice_4  \\\n",
       "0                           V               T               U               E   \n",
       "1  ICE FROM THE MOUNTAIN PEAK         AS RAIN            WIND           GRASS   \n",
       "2                           W               J               H               T   \n",
       "3                           W               A               H               J   \n",
       "4                           T               H               A               W   \n",
       "\n",
       "  correct_answer                                         image_path  \\\n",
       "0              E  ../Dataset/test/abc_question_images/rain_shado...   \n",
       "1        AS RAIN  ../Dataset/test/abc_question_images/rain_shado...   \n",
       "2              T  ../Dataset/test/abc_question_images/rain_shado...   \n",
       "3              J  ../Dataset/test/abc_question_images/rain_shado...   \n",
       "4              T  ../Dataset/test/abc_question_images/rain_shado...   \n",
       "\n",
       "  image_has_labels_to_guess                       caption  \\\n",
       "0                       Yes  a diagram of the water cycle   \n",
       "1                       Yes  a diagram of the water cycle   \n",
       "2                       Yes  a diagram of the water cycle   \n",
       "3                       Yes  a diagram of the water cycle   \n",
       "4                       Yes  a diagram of the water cycle   \n",
       "\n",
       "  gpt4_generated_answers  \n",
       "0                   None  \n",
       "1                   None  \n",
       "2                   None  \n",
       "3                   None  \n",
       "4                   None  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset containing images and associated questions\n",
    "image_qa_df = pd.read_csv(\"../Dataset/test/DiagramQuestionsData.csv\")\n",
    "image_qa_df[\"gpt4_generated_answers\"] = None\n",
    "image_qa_df.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "image_qa_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# Prompt GPT for VQA task\n",
    "def prompt_gpt(text, base64_image):\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {openai_api_key}\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4-vision-preview\",\n",
    "        \"messages\": [\n",
    "          {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "              {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": text\n",
    "              },\n",
    "              {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                  \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                }\n",
    "              }\n",
    "            ]\n",
    "          }\n",
    "        ],\n",
    "        \"max_tokens\": 300\n",
    "    }\n",
    "\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Dataset/test/question_images/food_chains_webs_6059.png\n",
      "{'error': {'message': 'Rate limit reached for gpt-4-vision-preview in organization org-lKFR4bNO94Tw5RDncWUcSIje on requests per day (RPD): Limit 100, Used 100, Requested 1. Please try again in 14m24s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Rate limit exceed. Exiting loop...\n"
     ]
    }
   ],
   "source": [
    "# Retrieve results from GPT-4 by looping through all questions\n",
    "for idx, question in enumerate(image_qa_df[\"question_name\"]):\n",
    "    if pd.isna(image_qa_df.loc[idx, \"gpt4_generated_answers\"]):\n",
    "        # Build answer choice string\n",
    "        answer_choice_1 = image_qa_df.loc[idx, \"answer_choice_1\"]\n",
    "        answer_choice_2 = image_qa_df.loc[idx, \"answer_choice_2\"]\n",
    "        answer_choice_3 = image_qa_df.loc[idx, \"answer_choice_3\"]\n",
    "        answer_choice_4 = image_qa_df.loc[idx, \"answer_choice_4\"]\n",
    "        answer_string = f\"{answer_choice_1}\\n{answer_choice_2}\\n{answer_choice_3}\\n{answer_choice_4}\"\n",
    "        \n",
    "        # Get base64 string version of the image\n",
    "        base64_image = encode_image(image_qa_df.loc[idx, \"image_path\"])\n",
    "        \n",
    "        # Build prompt\n",
    "        prompt = f\"Choose only one option below as the answer for the following question. An explanation is not needed.\\nQuestion: {question}\\n\\n{answer_string}\"\n",
    "        gpt_json_result = prompt_gpt(prompt, base64_image)\n",
    "        \n",
    "        if (\"error\" in gpt_json_result):\n",
    "            print(gpt_json_result)\n",
    "            # Rate Limits can be reached, hence we have to manually run this function multiple times in order\n",
    "            # to get all the results of our MCQ questions in the dataset\n",
    "            print(\"Rate limit exceed. Exiting loop...\")\n",
    "            break\n",
    "        image_qa_df.loc[idx, \"gpt4_generated_answers\"] = gpt_json_result[\"choices\"][0][\"message\"][\"content\"]\n",
    "        image_qa_df.to_csv(\"GPT_4_preds.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correct predictions: 2520 out of a total of 3285 questions.\n",
      "Accuracy: 76.71232876712328\n"
     ]
    }
   ],
   "source": [
    "# Calculate Accuracy of GPT-4 predictions\n",
    "image_qa_df = pd.read_csv(\"GPT_4_preds.csv\")\n",
    "correct_preds = image_qa_df[image_qa_df[\"correct_answer\"].str.lower() == image_qa_df[\"gpt4_generated_answers\"].str.lower()]\n",
    "print(f\"Number of correct predictions: {len(correct_preds)} out of a total of {len(image_qa_df)} questions.\")\n",
    "print(f\"Accuracy: {len(correct_preds)/len(image_qa_df) * 100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Focusing on True/False questions: </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function created using ChatGPT. Prompt used was: Write a python function for sending post requests to GPT-4's \n",
    "# completions API so that we can ask questions to it.\n",
    "def create_completion(prompt):\n",
    "    url = \"https://api.openai.com/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {openai_api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": \"gpt-4\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"max_tokens\": 150,\n",
    "        \"temperature\": 0.5,\n",
    "        \"top_p\": 1\n",
    "    }\n",
    "    response = requests.post(\n",
    "        url,\n",
    "        headers=headers,\n",
    "        data=json.dumps(data)\n",
    "    )\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>lesson_name</th>\n",
       "      <th>question_name</th>\n",
       "      <th>answer_choice_1</th>\n",
       "      <th>answer_choice_2</th>\n",
       "      <th>correct_answer</th>\n",
       "      <th>gpt_generated_answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>the nature of science</td>\n",
       "      <td>The scientific method is used to answer any qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>the nature of science</td>\n",
       "      <td>Scientific models are an organized step-by-ste...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>the nature of science</td>\n",
       "      <td>The dependent variable in an experiment is dir...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>the nature of science</td>\n",
       "      <td>Even if there is information we dont know, a m...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>the nature of science</td>\n",
       "      <td>A theory will still remain even if conflicting...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            lesson_name  \\\n",
       "0           0  the nature of science   \n",
       "1           1  the nature of science   \n",
       "2           2  the nature of science   \n",
       "3           3  the nature of science   \n",
       "4           4  the nature of science   \n",
       "\n",
       "                                       question_name  answer_choice_1  \\\n",
       "0  The scientific method is used to answer any qu...             True   \n",
       "1  Scientific models are an organized step-by-ste...             True   \n",
       "2  The dependent variable in an experiment is dir...             True   \n",
       "3  Even if there is information we dont know, a m...             True   \n",
       "4  A theory will still remain even if conflicting...             True   \n",
       "\n",
       "   answer_choice_2  correct_answer gpt_generated_answers  \n",
       "0            False           False                  None  \n",
       "1            False           False                  None  \n",
       "2            False            True                  None  \n",
       "3            False            True                  None  \n",
       "4            False           False                  None  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get True/False answers\n",
    "true_false_df = pd.read_csv(\"../Dataset/test/NonDiagram_True_False_QuestionsData.csv\")\n",
    "true_false_df[\"gpt4_generated_answers\"] = None\n",
    "true_false_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve results from GPT-4 by looping through all questions\n",
    "for idx, question in enumerate(true_false_df[\"question_name\"]):\n",
    "    if pd.isna(true_false_df.loc[idx, \"gpt4_generated_answers\"]):\n",
    "        prompt = f\"Is this statement True or False: \\\"{question}\\\". Only tell me if its True or False. An explanation is not required.\"\n",
    "        gpt_json_result = create_completion(prompt)\n",
    "        if (\"error\" in gpt_json_result):\n",
    "            print(gpt_json_result)\n",
    "            # Rate Limits can be reached, hence we have to manually run this function multiple times in order\n",
    "            # to get all the results of our true/false questions in the dataset\n",
    "            print(\"Rate limit exceed. Exiting loop...\")\n",
    "            break\n",
    "        true_false_df.loc[idx, \"gpt4_generated_answers\"] = gpt_json_result[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write results to CSV file\n",
    "true_false_df.to_csv(\"../Dataset/test/NonDiagram_True_False_QuestionsData.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 88.71%\n"
     ]
    }
   ],
   "source": [
    "# Find accuracy of True/False questions\n",
    "num_correct_guesses = 0\n",
    "for expected, actual in zip(true_false_df[\"correct_answer\"], true_false_df[\"gpt4_generated_answers\"]):\n",
    "    if actual == str(expected):\n",
    "        num_correct_guesses += 1\n",
    "\n",
    "print(f\"Accuracy = {(num_correct_guesses / true_false_df.shape[0]) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Now we focus on non-diagram questions: </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lesson_name</th>\n",
       "      <th>question_name</th>\n",
       "      <th>answer_choice_1</th>\n",
       "      <th>answer_choice_2</th>\n",
       "      <th>answer_choice_3</th>\n",
       "      <th>answer_choice_4</th>\n",
       "      <th>correct_answer</th>\n",
       "      <th>gpt4_generated_answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the nature of science</td>\n",
       "      <td>Steps of the scientific method include all of ...</td>\n",
       "      <td>doing background research.</td>\n",
       "      <td>constructing a hypothesis.</td>\n",
       "      <td>asking a question.</td>\n",
       "      <td>proving a theory.</td>\n",
       "      <td>proving a theory.</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the nature of science</td>\n",
       "      <td>Why do scientists call the Big Bang a theory?</td>\n",
       "      <td>It is probably unlikely and therefore not a fact.</td>\n",
       "      <td>A very well respected scientist proved it to b...</td>\n",
       "      <td>Many scientists have agreed upon this explanat...</td>\n",
       "      <td>All possible answers to a scientific idea are ...</td>\n",
       "      <td>Many scientists have agreed upon this explanat...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the nature of science</td>\n",
       "      <td>The data collected in an experiment should alw...</td>\n",
       "      <td>labeled.</td>\n",
       "      <td>recorded.</td>\n",
       "      <td>reported.</td>\n",
       "      <td>all of the above</td>\n",
       "      <td>all of the above</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the nature of science</td>\n",
       "      <td>Which of the following is not a scientific model?</td>\n",
       "      <td>A cross section of an apple that mimics the la...</td>\n",
       "      <td>A chart with nutritional information about foo...</td>\n",
       "      <td>A computer simulation that can show what will ...</td>\n",
       "      <td>An explanation for the extinction of the dinos...</td>\n",
       "      <td>A chart with nutritional information about foo...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the nature of science</td>\n",
       "      <td>If the results of an experiment disprove a hyp...</td>\n",
       "      <td>results should not be reported.</td>\n",
       "      <td>hypothesis is just a theory.</td>\n",
       "      <td>data must contain errors.</td>\n",
       "      <td>none of the above</td>\n",
       "      <td>none of the above</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             lesson_name                                      question_name  \\\n",
       "0  the nature of science  Steps of the scientific method include all of ...   \n",
       "1  the nature of science      Why do scientists call the Big Bang a theory?   \n",
       "2  the nature of science  The data collected in an experiment should alw...   \n",
       "3  the nature of science  Which of the following is not a scientific model?   \n",
       "4  the nature of science  If the results of an experiment disprove a hyp...   \n",
       "\n",
       "                                     answer_choice_1  \\\n",
       "0                         doing background research.   \n",
       "1  It is probably unlikely and therefore not a fact.   \n",
       "2                                           labeled.   \n",
       "3  A cross section of an apple that mimics the la...   \n",
       "4                    results should not be reported.   \n",
       "\n",
       "                                     answer_choice_2  \\\n",
       "0                         constructing a hypothesis.   \n",
       "1  A very well respected scientist proved it to b...   \n",
       "2                                          recorded.   \n",
       "3  A chart with nutritional information about foo...   \n",
       "4                       hypothesis is just a theory.   \n",
       "\n",
       "                                     answer_choice_3  \\\n",
       "0                                 asking a question.   \n",
       "1  Many scientists have agreed upon this explanat...   \n",
       "2                                          reported.   \n",
       "3  A computer simulation that can show what will ...   \n",
       "4                          data must contain errors.   \n",
       "\n",
       "                                     answer_choice_4  \\\n",
       "0                                  proving a theory.   \n",
       "1  All possible answers to a scientific idea are ...   \n",
       "2                                   all of the above   \n",
       "3  An explanation for the extinction of the dinos...   \n",
       "4                                  none of the above   \n",
       "\n",
       "                                      correct_answer gpt4_generated_answers  \n",
       "0                                  proving a theory.                   None  \n",
       "1  Many scientists have agreed upon this explanat...                   None  \n",
       "2                                   all of the above                   None  \n",
       "3  A chart with nutritional information about foo...                   None  \n",
       "4                                  none of the above                   None  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_diagram_qa_df = pd.read_csv(\"../Dataset/test/NonDiagram_MCQ_QuestionsData.csv\")\n",
    "non_diagram_qa_df.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "non_diagram_qa_df[\"gpt4_generated_answers\"] = None\n",
    "non_diagram_qa_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve results from GPT-4 by looping through all questions\n",
    "for idx, question in enumerate(non_diagram_qa_df[\"question_name\"]):\n",
    "    if pd.isna(non_diagram_qa_df.loc[idx, \"gpt4_generated_answers\"]):\n",
    "        # Build answer choice string\n",
    "        answer_choice_1 = non_diagram_qa_df.loc[idx, \"answer_choice_1\"]\n",
    "        answer_choice_2 = non_diagram_qa_df.loc[idx, \"answer_choice_2\"]\n",
    "        answer_choice_3 = non_diagram_qa_df.loc[idx, \"answer_choice_3\"]\n",
    "        answer_choice_4 = non_diagram_qa_df.loc[idx, \"answer_choice_4\"]\n",
    "        answer_string = f\"{answer_choice_1}\\n{answer_choice_2}\\n{answer_choice_3}\\n{answer_choice_4}\"\n",
    "            \n",
    "        # Build prompt\n",
    "        prompt = f\"Choose only one option below as the answer for the following question. An explanation is not needed.\\nQuestion: {question}\\n\\n{answer_string}\"\n",
    "        gpt_json_result = create_completion(prompt)\n",
    "        if (\"error\" in gpt_json_result):\n",
    "            print(gpt_json_result)\n",
    "            # Rate Limits can be reached, hence we have to manually run this function multiple times in order\n",
    "            # to get all the results of our MCQ questions in the dataset\n",
    "            print(\"Rate limit exceed. Exiting loop...\")\n",
    "            break\n",
    "        non_diagram_qa_df.loc[idx, \"gpt4_generated_answers\"] = gpt_json_result[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write results to CSV file\n",
    "non_diagram_qa_df.to_csv(\"../Dataset/test/NonDiagram_MCQ_QuestionsData.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 83.98%\n"
     ]
    }
   ],
   "source": [
    "# Find accuracy of Non-diagram MCQ questions\n",
    "num_correct_guesses = 0\n",
    "for expected, actual in zip(non_diagram_qa_df[\"correct_answer\"], non_diagram_qa_df[\"gpt4_generated_answers\"]):\n",
    "    if actual == expected:\n",
    "        num_correct_guesses += 1\n",
    "\n",
    "print(f\"Accuracy = {(num_correct_guesses / non_diagram_qa_df.shape[0]) * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
